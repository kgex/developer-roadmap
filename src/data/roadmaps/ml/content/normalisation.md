# Normalisation

## What is Normalisation?

Batch normalization (BN) is a technique used in machine learning and deep learning to improve the performance and stability of artificial neural networks. It involves normalizing the inputs to each layer of a neural network to have zero mean and unit variance, which can help to prevent overfitting and improve training speed.

## Why we use Normalisation?

Batch normalization has been shown to be effective in a wide range of deep learning tasks, including image classification, object detection, and s  peech recognition. It is widely used in many popular deep learning frameworks, such as TensorFlow, PyTorch, and Keras.

<br>

Visit the following resources to learn more:    
- [Feature Scaling and Normalization](https://machinelearningmastery.com/normalize-standardize-machine-learning-data-weka/)

- [What is Data Normalization and Why Is It Important?](https://dzone.com/articles/what-is-data-normalization-and-why-is-it-important)

- [Understanding Feature Scaling and Normalization in Machine Learning](https://www.datacamp.com/community/tutorials/feature-scaling-normalization-in-python)

- [Why, How and When to Scale Your Features](https://sebastianraschka.com/Articles/2014_about_feature_scaling.html)

- [Normalization vs Standardization](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/)

- [Learn Batch Normalisation in 20 mins](https://youtu.be/dXB-KQYkzNU)