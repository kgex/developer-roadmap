# ALBERT

The advent of BERT natural language research has embraced a new paradigm, leveraging large amounts of existing text to pretrain a modelâ€™s parameters using self-supervision, with no data annotation required.
So, rather than needing to train a machine-learning model for natural language processing (NLP) from scratch, one can start from a model primed with knowledge of a language. 
ALBERT is being released as an open-source implementation on top of TensorFlow.


# Learning Sources

[concept](https://www.youtube.com/watch?v=Lwtj2yUAMgI)

[Albert Model](https://www.analyticsvidhya.com/blog/2022/10/albert-model-for-self-supervised-learning/)

[Blogs](https://sh-tsang.medium.com/review-albert-a-lite-bert-for-self-supervised-learning-of-language-representations-14e1fcc05ba9)

[Paper Summary](https://amitness.com/2020/02/albert-visual-summary/)

